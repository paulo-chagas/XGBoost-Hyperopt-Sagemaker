{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone, datetime, timedelta\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dt_now = datetime.now(timezone(-timedelta(hours=3)))\n",
    "print(f'Started at {dt_now.strftime(\"%Y-%m-%d %H:%M\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hyperopt\n",
    "%pip install xgboost\n",
    "%pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization with hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from evaluation_utils import get_selected_features, get_domain_dict, get_search_space_hyperopt, mape_scorer, MAPE\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket = 'bucket_name'\n",
    "seed = 1995\n",
    "\n",
    "domain_dict = get_domain_dict()\n",
    "info_dict = {}\n",
    "\n",
    "for domain in domain_dict.keys():\n",
    "    print('='*50)\n",
    "    print(domain)\n",
    "    print('='*50)\n",
    "    TARGET = domain_dict[domain]['TARGET']\n",
    "    target_problem = domain_dict[domain]['target_problem']\n",
    "    # corr_threshold = domain_dict[domain]['corr_threshold']\n",
    "    # delay_days = domain_dict[domain]['delay_days']\n",
    "\n",
    "    data = f's3://{bucket}/{target_problem}/train/data.csv'\n",
    "\n",
    "    df_full = pd.read_csv(data)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.sort_values(by='date', inplace=True, ascending=True, ignore_index=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # removing latest 15 days\n",
    "    df = df.loc[df['date'] < df['date'].max() - timedelta(days=15)].copy()\n",
    "    print(df.shape)\n",
    "\n",
    "    print('Selecting features')\n",
    "    # corrmat = df.corr()\n",
    "    # top_corr_features = corrmat.index[abs(corrmat[TARGET])>=corr_threshold]\n",
    "    # top_corr_features = top_corr_features.to_list()\n",
    "    # print(top_corr_features)\n",
    "    top_corr_features = list(get_selected_features(df, TARGET))\n",
    "    print(f'Done. Selected {len(top_corr_features)} features')\n",
    "\n",
    "    if len(top_corr_features) == 0:\n",
    "        print('0 features chosen\\nExit')\n",
    "        sys.exit(0)\n",
    "\n",
    "    full_columns = top_corr_features[:]\n",
    "    full_columns.insert(0, 'date')\n",
    "    full_columns.append(TARGET)\n",
    "    df_preprocess = df[full_columns].copy()\n",
    "    # upscaling small values (percentages)\n",
    "    df_preprocess[TARGET] = df_preprocess[TARGET] * 100\n",
    "    print('\\nMultiplying low-scale values')\n",
    "    for col in top_corr_features:\n",
    "        if 'pct_' in col:\n",
    "            df_preprocess[col] = df_preprocess[col]*100\n",
    "    df_preprocess.to_csv(\n",
    "        f's3://{bucket}/{target_problem}/preprocess/train_preprocess.csv', index=False, header=True)\n",
    "\n",
    "    X = df_preprocess[top_corr_features].copy()\n",
    "    y = df_preprocess[TARGET].copy()\n",
    "\n",
    "    print('Date interval: ')\n",
    "    print(df_preprocess['date'].min())\n",
    "    print(df_preprocess['date'].max())\n",
    "    print(f'{X.shape[0]} instances - {X.shape[1]} features')\n",
    "\n",
    "    X.fillna(0, inplace=True)\n",
    "    standard_scaler = StandardScaler()\n",
    "\n",
    "    results = {}\n",
    "    estimators = get_search_space_hyperopt()\n",
    "    for model_name in estimators:\n",
    "        results[model_name] = {\"scores\": list()}\n",
    "\n",
    "    # configure the cross-validation procedure\n",
    "    k = 5\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    # kfold = TimeSeriesSplit(n_splits=k)\n",
    "    if isinstance(kfold, KFold):\n",
    "        print(f'{k}-fold CV')\n",
    "        print(f'Test size = {X.shape[0]/k}')\n",
    "    elif isinstance(kfold, TimeSeriesSplit):\n",
    "        print(f'{k}-fold Time Series Split')\n",
    "        print(f'Test size = {X.shape[0]//(k + 1)}')\n",
    "\n",
    "    for model_name in estimators:\n",
    "        if estimators[model_name][\"skip\"]:\n",
    "            continue\n",
    "        print('='*50)\n",
    "        print(f'Model {model_name}')\n",
    "        model = estimators[model_name][\"model\"]\n",
    "        params = estimators[model_name][\"space\"]\n",
    "        reduce = estimators[model_name][\"reduce\"]\n",
    "\n",
    "        def objective(params):\n",
    "            clf = model(**params)\n",
    "            pipeline = Pipeline(\n",
    "                [('transformer', standard_scaler), ('estimator', clf)])\n",
    "            # Extract the best score\n",
    "            scores = -cross_val_score(pipeline, X, y, cv=kfold,\n",
    "                                      scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "            # trying no minimize the average\n",
    "            avg_score = reduce(scores) \n",
    "\n",
    "            return {'loss': avg_score, 'status': STATUS_OK}\n",
    "\n",
    "        trials = Trials()\n",
    "        best_hyperparams = fmin(fn=objective,\n",
    "                                space=params,\n",
    "                                algo=tpe.suggest,\n",
    "                                max_evals=500,\n",
    "                                trials=trials,\n",
    "                                rstate=np.random.default_rng(seed)\n",
    "                                )\n",
    "        print(f'Best hyperparameters: {space_eval(params, best_hyperparams)}')\n",
    "        clf = model().set_params(**space_eval(params, best_hyperparams))\n",
    "        print(clf)\n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            [('transformer', standard_scaler), ('estimator', clf)])\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X, y, cv=kfold, scoring=mape_scorer, n_jobs=-1)\n",
    "        print(scores)\n",
    "        print(f'{np.mean(scores)} +- ({np.std(scores)})')\n",
    "\n",
    "        results[model_name][\"scores\"] = scores.tolist()\n",
    "        results[model_name][\"mean_scores\"] = np.mean(scores)\n",
    "        results[model_name][\"std_scores\"] = np.std(scores)\n",
    "        # results[model_name][\"corr_threshold\"] = corr_threshold\n",
    "        results[model_name][\"feature_columns\"] = top_corr_features\n",
    "        results[model_name][\"hyperparameters\"] = space_eval(\n",
    "            params, best_hyperparams)\n",
    "        results[model_name][\"trained_at\"] = datetime.today().strftime(\n",
    "            '%Y-%m-%d___%H-%M-%S')\n",
    "\n",
    "    info_dict[domain] = results['xgboost']\n",
    "\n",
    "    s3_client.put_object(\n",
    "            Body=json.dumps(results, indent=4),\n",
    "            Bucket=bucket,\n",
    "            Key=f'{target_problem}/experiments/current.json'\n",
    "    )\n",
    "    print(json.dumps(results, indent=4))\n",
    "\n",
    "s3_client.put_object(\n",
    "        Body=json.dumps(info_dict, indent=4),\n",
    "        Bucket=bucket,\n",
    "        Key=f'xgboost_info.json'\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(f'Elapsed time (seconds): {elapsed}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_client = boto3.client('sagemaker', region_name='us-east-1')\n",
    "\n",
    "for domain in domain_dict.keys():\n",
    "    print('='*50)\n",
    "    print(domain)\n",
    "    print('='*50)\n",
    "    TARGET = domain_dict[domain]['TARGET']\n",
    "    target_problem = domain_dict[domain]['target_problem']\n",
    "\n",
    "    content_object = s3_resource.Object(\n",
    "        bucket, f'{target_problem}/experiments/current.json')\n",
    "    file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "    json_content = json.loads(file_content)\n",
    "    hyperparameters = json_content['xgboost']['hyperparameters']\n",
    "    feature_columns = json_content['xgboost']['feature_columns']\n",
    "    print(json.dumps(hyperparameters, sort_keys=True, indent=4))\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        f's3://{bucket}/{target_problem}/preprocess/train_preprocess.csv')\n",
    "    X = df[feature_columns].copy()\n",
    "    y = df[TARGET].copy()\n",
    "\n",
    "    X.fillna(0, inplace=True)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = pd.DataFrame(scaler.transform(X), columns=feature_columns)\n",
    "\n",
    "    pd.concat([y, X], axis=1).to_csv(\n",
    "        f's3://{bucket}/{target_problem}/xgboost/train/train.csv', index=False, header=False)\n",
    "    s3_input_train = sagemaker.TrainingInput(\n",
    "        f's3://{bucket}/{target_problem}/xgboost/train/train.csv', content_type='csv')\n",
    "\n",
    "    container = sagemaker.image_uris.retrieve(\n",
    "        'xgboost', region=boto3.Session().region_name, version='1.0-1')\n",
    "\n",
    "    json_content['xgboost']['container'] = str(container)\n",
    "    s3_client.put_object(\n",
    "        Body=json.dumps(json_content),\n",
    "        Bucket=bucket,\n",
    "        Key=f'{target_problem}/experiments/current.json'\n",
    "    )\n",
    "\n",
    "    model_output = f's3://{bucket}/{target_problem}/xgboost/models/'\n",
    "\n",
    "    n_round = hyperparameters['n_estimators']\n",
    "    xgb_estimator = sagemaker.estimator.Estimator(image_uri=container,\n",
    "                                                  hyperparameters=hyperparameters,\n",
    "                                                  role=role,\n",
    "                                                  instance_count=1,\n",
    "                                                  instance_type='ml.m4.xlarge',\n",
    "                                                  output_path=model_output\n",
    "                                                  )\n",
    "    xgb_estimator.set_hyperparameters(objective='reg:squarederror',\n",
    "                                      num_round=n_round\n",
    "                                      )\n",
    "    xgb_estimator.fit({'train': s3_input_train})\n",
    "\n",
    "    primary_container = {'Image': container,\n",
    "                         'ModelDataUrl': xgb_estimator.model_data\n",
    "                         }\n",
    "\n",
    "    try:\n",
    "        delete_response = sagemaker_client.delete_model(\n",
    "            ModelName=target_problem)\n",
    "    except Exception as e:\n",
    "        print('Could not delete model')\n",
    "        print(e)\n",
    "\n",
    "    create_model_response = sagemaker_client.create_model(\n",
    "        ModelName=target_problem,\n",
    "        ExecutionRoleArn=role,\n",
    "        PrimaryContainer=primary_container)\n",
    "    print(f\"myINFO : Created Model ARN : {create_model_response['ModelArn']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed = time.time() - start_time\n",
    "print(f'Elapsed time (seconds): {elapsed}')\n",
    "\n",
    "info_dict['total_processing_time'] = elapsed\n",
    "\n",
    "s3_client.put_object(\n",
    "        Body=json.dumps(info_dict, indent=4),\n",
    "        Bucket=bucket,\n",
    "        Key=f'xgboost_info.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker', region_name='us-east-1')\n",
    "sagemaker_client.stop_notebook_instance(\n",
    "        NotebookInstanceName='schedule-training-notebook')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b84fcef57f3a6ff75c195672ae51a7d9ebf6bf3ec55cb02c9f07a23ba236e6de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
